{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LLM to evalute the generate question and filter the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "你将获得上下文内容和问题。\n",
    "您的任务是提供“评分”，以评估在给定上下文中明确回答给定问题的能力。\n",
    "按照 1 到 5 的分数给出答案，其中 1 表示该问题在给定上下文的情况下根本无法回答，5 表示该问题可以根据上下文清楚、明确地回答。\n",
    "\n",
    "请按照如下方式提供答案：\n",
    "\n",
    "答案::: \n",
    "评估理由:(你评分的理由，以文本形式) \n",
    "评分:(你的评分，以 1 到 5 之间的数字表示）\n",
    "\n",
    "你必须在答案中提供“评估理由”和“评分”。\n",
    "\n",
    "下面是问题和上下文。\n",
    "\n",
    "问题:{question}\\n\n",
    "上下文:{context}\\n\n",
    "答案::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "你将收到一个问题。\n",
    "你的任务是提供“评分”，表示该问题对于一个普通公民对理解政策文件有多大用处。\n",
    "按照 1 到 5 的范围给出答案，其中 1 表示该问题根本没有用，5 表示该问题非常有用。\n",
    "\n",
    "请按照如下方式提供你的答案： \n",
    "\n",
    "答案::: \n",
    "评估理由:(你评分的理由，以文本形式) \n",
    "评分:(你的评分，以 1 到 5 之间的数字表示）\n",
    "\n",
    "你必须在答案中提供“评估理由”和“评分”。\n",
    "\n",
    "下面是问题。\n",
    "\n",
    "问题:{question}\\n\n",
    "答案::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "您将收到一个问题。\n",
    "您的任务是提供一个“评分”，表示该问题与上下文的独立程度。\n",
    "按照 1 到 5 的范围给出答案，其中 1 表示问题依赖于其他信息才能理解，5 表示问题本身有意义。\n",
    "例如，如果问题涉及特定设置，例如“在上下文中”或“该文件”，则评级必须为 1。\n",
    "这些问题可以包含一些特定的指代，例如我市，但仍然是 5：必须让有权访问文件的人员清楚问题的内容。\n",
    "\n",
    "例如：“该文件实施的背景是什么？” 应该评分为1，因为隐含地提到了上下文，因此该问题并不是独立于上下文的。\n",
    "\n",
    "请按如下方式提供您的答案：\n",
    "\n",
    "答案::: \n",
    "评估理由:(你评分的理由，以文本形式) \n",
    "评分:(你的评分，以 1 到 5 之间的数字表示）\n",
    "\n",
    "你必须在答案中提供“评估理由”和“评分”。\n",
    "\n",
    "下面是问题。\n",
    "\n",
    "问题:{question}\\n\n",
    "答案::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "path = r'RAG_PLUS\\generate_qa_couples\\batchQA_output'\n",
    "\n",
    "path_list = os.listdir(path)\n",
    "for index, doc in enumerate(path_list):\n",
    "    real_path = os.path.join(path, doc)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(real_path)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    outputs = df.to_dict(orient='records')\n",
    "\n",
    "    print(f\"{index}doc---Generating critique for each QA couple...\")\n",
    "    for output in tqdm(outputs):\n",
    "        time.sleep(2)\n",
    "        evaluations = {\n",
    "            \"groundedness\": ask_glm(question_groundedness_critique_prompt.format(context=output['context'], question=output['question']))['choices'][0]['message']['content'],\n",
    "            \"relevance\": ask_glm(question_relevance_critique_prompt.format(question=output['question']))['choices'][0]['message']['content'],\n",
    "            \"standalone\": ask_glm(question_standalone_critique_prompt.format(question=output['question']))['choices'][0]['message']['content']\n",
    "        }\n",
    "        try:\n",
    "            for criterion, evaluation in evaluations.items():\n",
    "                evaluation = evaluation.strip().replace(':','').replace(\"：\",'').replace(\"-\", '').replace(\"\\n\", '')\n",
    "                print(evaluation)\n",
    "                score, eval_res = (\n",
    "                    int(evaluation.split(\"评分\")[-1].strip()),\n",
    "                    evaluation.split(\"评分\")[-2].split(\"评估理由\")[1].strip(),\n",
    "                )\n",
    "                output.update(\n",
    "                    {\n",
    "                        f\"{criterion}_score\": score,\n",
    "                        f\"{criterion}_eval\": eval_res,\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(outputs)\n",
    "    df.to_csv(f'./glm_evaluate_q/{index}doc.csv', index=False, encoding='utf-8-sig')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat and filter the rating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat 所有的评分数据\n",
    "import pandas as pd\n",
    "import os\n",
    "dir_path = r\"C:\\Users\\Lenovo\\Desktop\\CUHK\\CUHK_2024\\Course\\Project\\RAG_Plus\\glm_evaluate_q\"\n",
    "doc_frames = []\n",
    "doc_path_list = os.listdir(dir_path)\n",
    "for doc_name in doc_path_list:\n",
    "    doc_path = os.path.join(dir_path, doc_name)\n",
    "    doc_csv = pd.read_csv(doc_path)\n",
    "    doc_frames.append(doc_csv)\n",
    "\n",
    "combined_df = pd.concat(doc_frames, ignore_index=True)\n",
    "combined_df.to_csv(\"evaluate_qa_pair.csv\", index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "generated_questions = pd.read_csv(\"evaluate_qa_pair.csv\")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 3)\n",
    "]\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split='train', preserve_index=False)\n",
    "generated_questions.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMDev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
