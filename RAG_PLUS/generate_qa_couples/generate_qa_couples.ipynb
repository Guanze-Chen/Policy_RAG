{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('work_policy_data.json', 'r', encoding='utf-8') as json_file:\n",
    "    raw_data = json.load(json_file)\n",
    "    data = raw_data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Policy Question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Split documents to text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_doc(doc, chunk_size=300, chunk_overlap=30):\n",
    "\n",
    "    langchain_docs = []\n",
    "\n",
    "    # 转变为Langchain document 对象\n",
    "    combined_content = f\"{doc['metadata']['title']}\\n{doc['content']}\\n\" + \\\n",
    "                    \"\\n\".join([f\"问：{qa['question']} 答：{qa['answer']}\" for qa in doc['T_qa_pairs'] if doc['metadata']['qa_url'] != ''])\n",
    "    langchain_doc = LangchainDocument(page_content=combined_content, metadata=doc['metadata'])\n",
    "    langchain_docs.append(langchain_doc)\n",
    "\n",
    "    # 初始化文本分割器\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    # 处理文档 这个处理的话是把所有文档都汇集到一起了\n",
    "    docs_processed = []\n",
    "\n",
    "    for doc in langchain_docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "    \n",
    "    return docs_processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 load LLM API and prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import jwt\n",
    "import requests\n",
    "\n",
    "ZP_key = os.getenv(\"GLM_KEY\")\n",
    "def generate_token(apikey: str, exp_seconds: int):\n",
    "    try:\n",
    "        id, secret = apikey.split(\".\")\n",
    "    except Exception as e:\n",
    "        raise Exception(\"invalid apikey\", e)\n",
    "\n",
    "    payload = {\n",
    "        \"api_key\": id,\n",
    "        \"exp\": int(round(time.time() * 1000)) + exp_seconds * 1000,\n",
    "        \"timestamp\": int(round(time.time() * 1000)),\n",
    "    }\n",
    "\n",
    "    return jwt.encode(\n",
    "        payload,\n",
    "        secret,\n",
    "        algorithm=\"HS256\",\n",
    "        headers={\"alg\": \"HS256\", \"sign_type\": \"SIGN\"},\n",
    "    )\n",
    "\n",
    "def ask_glm(content):\n",
    "    url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'Authorization': generate_token(ZP_key, 1000)\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"glm-3-turbo\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": content}]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "# GPT-4 生成的prompt 融合自己的\n",
    "example_question = \"本办法的施行时间和有效期限是多久？\"\n",
    "prompt = \"\"\"\n",
    "你的任务是根据上下文写出一个事实问题和答案。\n",
    "你的事实陈述问题应该用来自上下文的具体、简洁的事实信息来回答。\n",
    "你的事实陈述问题应该采用与用户在进行政策问答提出的问题相同的风格。\n",
    "这意味着你的事实问题不能提及“根据段落”或“上下文”之类的内容。\n",
    "\n",
    "提供你的答案如下：\n",
    "输出:::\n",
    "事实陈述问题-(你的事实陈述问题)\n",
    "答案-(你对事实陈述问题的答案)\n",
    "\n",
    "这是上下文。\n",
    "\n",
    "上下文：{context}\\n\n",
    "输出:::\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Batch generate QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time \n",
    "import pandas as pd\n",
    "\n",
    "def generate_qa_couples(data, batch_size=5, chunksize=300, chunkoverlap=30, max_generate=15):\n",
    "    \"\"\"\n",
    "    batch_size: the number of docs processed once\n",
    "    chunk_size: the chunk of split doc\n",
    "    chunk_overlap: the overlop of every split doc\n",
    "    max_generate: the maximum qa couples of every policy file\n",
    "    \"\"\"\n",
    "    iter_times = int(len(data)) / batch_size\n",
    "    start_index = 0\n",
    "    end_index = batch_size\n",
    "    for batch in range(iter_times):\n",
    "        if batch < iter_times:\n",
    "            for index, doc in enumerate(data[start_index:end_index]):\n",
    "                docs_processed = split_doc(doc, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "                N_Generations = min(int(len(docs_processed) * (1-chunk_overlap / chunk_size)), max_generate)\n",
    "                llm_generate_questions(docs_processed, N_Generations , index, start_index)\n",
    "            start_index += batch_size\n",
    "            end_index += batch_size\n",
    "        else:\n",
    "            for index, doc in enumerate(data[end_index:]):\n",
    "                docs_processed = split_doc(doc, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "                N_Generations = min(int(len(docs_processed) * (1-chunk_overlap / chunk_size)), max_generate, end_index)\n",
    "                llm_generate_questions(docs_processed, N_Generations , index)\n",
    "\n",
    "\n",
    "\n",
    "def llm_generate_questions(docs_processed, N_Generations, index, start_index):\n",
    "    print(f\"Generating {N_Generations} QA couples...\")\n",
    "    outputs = []\n",
    "    for sampled_context in tqdm(random.sample(docs_processed, N_Generations)):\n",
    "        time.sleep(random.random()*3)\n",
    "        output_QA_couple = ask_glm(prompt.format(context=sampled_context.page_content))['choices'][0]['message']['content']\n",
    "        try:\n",
    "            question = output_QA_couple.split('事实陈述问题-')[-1].split('答案-')[0]\n",
    "            answer = output_QA_couple.split('答案-')[-1]\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"context\": sampled_context.page_content,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"source_url\": sampled_context.metadata['url'],\n",
    "                    \"category\": sampled_context.metadata['topic'],\n",
    "                    \"title\": sampled_context.metadata['title'],\n",
    "                    \"org\": sampled_context.metadata['source']\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    df = pd.DataFrame(outputs)\n",
    "    df.to_csv(f'./batchQA_output/{index+start_index}doc.csv', index=False, encoding='utf-8-sig')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMDev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
